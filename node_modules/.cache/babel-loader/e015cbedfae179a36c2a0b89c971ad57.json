{"ast":null,"code":"var _jsxFileName = \"/Users/brian/Desktop/CS4641/speech-activity-recognition/src/pages/FinalUpdate/FinalUpdate.js\",\n    _s = $RefreshSig$();\n\nimport React from 'react';\nimport { Typography, Box } from '@material-ui/core';\nimport { makeStyles } from '@material-ui/core/styles';\nimport { Header } from '../../components'; //import { Link } from 'react-router-dom';\n\nimport teaser from \"../../img/teaser.png\";\nimport oneHot from \"../../img/oneHot.png\";\nimport softmax from \"../../img/softmax.png\";\nimport model from \"../../img/model.png\";\nimport train from \"../../img/train.png\";\nimport epoch from \"../../img/epoch.png\";\nimport correct from \"../../img/correct.png\";\nimport wrong from \"../../img/wrong.png\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst useStyles = makeStyles(theme => ({\n  wrapper: {\n    display: 'flex',\n    alignItems: 'center',\n    flexDirection: 'column',\n    paddingTop: '20px'\n  },\n  link: {\n    textDecoration: 'none',\n    color: '#000'\n  },\n  boxFormat: {\n    width: '70%' //paddingBottom:'20px',\n\n  },\n  titleFormat: {\n    paddingBottom: '10px',\n    textDecoration: \"none\",\n    color: '#212F3C' //fontFamily: '-apple-system',\n\n  },\n  lateTitleFormat: {\n    paddingTop: '20px',\n    paddingBottom: '10px',\n    textDecoration: \"none\",\n    color: '#212F3C' //fontFamily: '-apple-system',\n\n  },\n  titleParagraphFormat: {\n    fontFamily: '-apple-system'\n  },\n  titleParagraphFormat2: {\n    fontFamily: '-apple-system',\n    padding: '10px'\n  },\n  imageFormat: {//paddingBottom:'20px',\n  },\n  imageFormat2: {\n    paddingBottom: '30px'\n  },\n  imageFormat3: {\n    paddingTop: '20px'\n  }\n}));\n\nconst FinalUpdate = ({\n  tagChange\n}) => {\n  _s();\n\n  const classes = useStyles(); //const theme = useTheme();\n\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    className: classes.wrapper,\n    children: [/*#__PURE__*/_jsxDEV(Header, {}, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 72,\n      columnNumber: 13\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: [/*#__PURE__*/_jsxDEV(Typography, {\n        align: \"center\",\n        variant: \"h4\",\n        className: classes.titleFormat,\n        children: \"Abstract\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 76,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat,\n        children: [\"The goal of our project is to train a convolutional neural network (CNN) to detect the digits of a house number displayed on the streets. This is a basic image recognition that can be useful in many different fields. We were able to train our neural network to perform multi-digit recognition with an accuracy rate of 94.2%.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 81,\n          columnNumber: 203\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 81,\n          columnNumber: 212\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 79,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 73,\n      columnNumber: 13\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat,\n      src: teaser,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 84,\n      columnNumber: 13\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: [/*#__PURE__*/_jsxDEV(Typography, {\n        align: \"center\",\n        variant: \"h4\",\n        className: classes.lateTitleFormat,\n        children: \"Introduction\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 86,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: \"As shown in the image above, our project targets images of house numbers displayed on the streets, we hope to achieve reasonable results given a variety of resolutions of the images. Digit recognition is becoming increasingly important in various domains as technology advances. Whether it is a mapping company needing to match images of house numbers to their geolocations, or a robot trying to locate itself through room numbers/house numbers, SVHN serves as a good basic dataset to dive into the world of image recognition. We wanted to implement our project from scratch to understand how to preprocess the dataset as well as to gain a deeper understanding of how to set up and train our own neural network. Most existing ways of recognizing street view house numbers include two steps after locating where the numbers are within a picture: slicing each digit and recognizing. The ways some other projects built the neural network rely on preprocessing the picture to make boxes around each digit, and can only recognize individual digits. The output layers of their convolutional neural networks have 11 neurons, each corresponding to a possible digit (ten of which represent zero to nine, and the last one represent none). Our approach only has one step after locating the numbers in the pictures. We directly feed the portion of the image with all the numbers into the network, and train the network to predict all of the numbers at the same time.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 89,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"center\",\n        variant: \"h4\",\n        className: classes.lateTitleFormat,\n        children: \"Approach\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 106,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: [\"The first step was to preprocess the data from \", /*#__PURE__*/_jsxDEV(\"a\", {\n          href: \"http://ufldl.stanford.edu/housenumbers/\",\n          children: \"The Street View House Numbers (SVHN) Dataset\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 110,\n          columnNumber: 64\n        }, this), \". We first focused on the first part of the dataset, where colored house-number images are given, and the bounding box for each digit is given in a .mat file. The general steps we took to preprocess the image dataset are to: limit the maximum number of digit to 5 (there is only one image that has 6 digits in the picture), make a new bounding box given the min and max values of the x and y values for all bounding boxes of all digits, expand the bounding box in each direction by 30% to ensure the coverage of all digit details. After preprocessing all the images of the dataset, we chose to use Tensorflow.Keras to help us construct the CNN as it has many predefined methods to build the architecture. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 110,\n          columnNumber: 867\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 110,\n          columnNumber: 876\n        }, this), \"As introduced in the midterm update, we were able to preprocess the images and store them in a .h5 file. The data contains cropped street view house number images of size 32x32 and their corresponding labels in the form of 5 elements lists. Each of these label lists contains 5 integers, ranging from 0 to 10, with 0 to 9 representing the actual number shown in the image, and 10 being a placeholder for images that have less than 5 numbers (If \\\"19\\\" appears in the image, the label list would be: [1, 9, 10, 10, 10]). To construct the CNN architecture, we began by defining the input and output layers. Our input would be the preprocessed images, which have size 32x32x3. Finding the way to construct the output layer was less intuitive for us. Common classification CNN selects and condenses important features in hidden layers, and makes the number of neurons on the output layer correspond to the number of classes the data belongs to. In this case, as each element in our labels ranges from 0 to 10, there should be 11 classes, which correspond to 11 neurons on the output layer, and the list containing the correct information for the output layer to compare with should contain 11 elements. This does not coincide with our label data set, as each of these label lists contain 5 elements. Therefore, we first further processed our labels and converted them using the one-hot encoding method. Each of these 5x1 label lists then becomes a 5x11 list, with ten 0s and one 1 located on the index corresponding to this label. Shown below is an example of how the label \\\"19\\\" is represented in the original label list as well as the one-hot format. \", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 111,\n          columnNumber: 1663\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 109,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 85,\n      columnNumber: 13\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: \"One-Hot Encoding\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 114,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat,\n      src: oneHot,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 115,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: [\"With the help of this, we further designed the output layer of the network to also be in the dimension of 5x11. We individually connected 5 separate softmax densely-connected layers with 11 neurons each to their previous common fully connected layer. Then we concatenated them and reshaped them into the size of 5x11.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 120,\n          columnNumber: 83\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 117,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 116,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: \"Design of Output Layer\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 123,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat2,\n      src: softmax,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 124,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: [\"After settling down our input and output layers, we started to construct the hidden layers. Considering that our dataset is only recognizing numbers, which do not have very complex featured shapes and patterns, but there are 11 possible categories, we decided to use 3 convolutional layers, each is followed by 3 pooling layers, and there are 2 fully connected layers at the end. For each of these convolutional layers, we used Rectified Linear Unit(ReLU) as our activation function, as it allows an arbitrary amount of output, which corresponds to our scenario of wanting to classify into many classes. The convolutional layers have 64, 128, 256 filters, with a 7x7, 5x5, 3x3 dimension of kernel respectively. As the dimension of the kernel represents the height and width of the 2D convolution window, we decided to gradually decrease it while increasing the amount of filters to capture more essential details. As for padding, we wanted the results coming out from the first layer to preserve spatial dimensions of the volume, so we set padding to be the same for the first layer. For the second and the third, as the layers gradually identified important features, we set the padding to valid to allow the spatial dimensions to reduce via the natural application of convolution.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 128,\n          columnNumber: 17\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 129,\n          columnNumber: 17\n        }, this), \"Before each of these convolutional layers, we performed a batch normalization to preprocess their input, reduce the internal covariate shift, and increase the learning efficiencies. Following each convolutional layer, we used Max Pooling with a size of 2x2 to reduce the spatial dimensions of the output volume. After this, we wanted to increase the challenges for the network to learn the data by randomly dropping out some neurons on each layer, so we made each of these max pooled layers dropout with a rate of [0.3, 0.3, 0.5] respectively. For the fully connected layers. We first flattened the data and then applied two fully connected layers with sizes 1024 and 512, using them to combine the features and prepare for the final output.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 131,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 126,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 125,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: [/*#__PURE__*/_jsxDEV(Typography, {\n        align: \"center\",\n        variant: \"h4\",\n        className: classes.lateTitleFormat,\n        children: \"Experimental and Qualitative Results\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 136,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: \"The input to the CNN is a set of 32x32x3 images, we conducted trials on black and white images first and found similar rates of success, we will be displaying the results of training and testing on colored images as this is more realistic and we do not expect users to preprocess images to black and white before using our model. Shown below is an image of the neural network model summary, showing the various layers and shapes.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 139,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 135,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: \"Model Summary\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 143,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat3,\n      src: model,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 144,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: \"After setting up our neural network as above, we spent a lot of time tuning the various things such as the number of drop out layers, the batch size, and the number of epochs ran. We found out that having more dropout layers increases the accuracy as the act of randomly dropping neurons increases the challenge for the network to learn the data, making the network record more essential details, and thus increasing its ability for generalization. The batch size was closely related to the number of epochs, as we realized that for a smaller batch size, a smaller value for epoch should be chosen for higher accuracy. We tested batch sizes of 16 and 64, varying the epoch value between 3, 19, 15, and 20. In the end, we chose a batch size of 64 and epoch value of 10 with the lowest loss towards the end and maximum accuracy. We tested two optimizers: Adam and Root Mean Square Propagation (RMSProp). We chose RMPSprop in the end as it tries to dampen the oscillations and automatically adjust the learning rates.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 146,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 145,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 150,\n        columnNumber: 21\n      }, this), \"Training Process\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 150,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat3,\n      src: train,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 151,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: \"As shown above, the accuracy increased as the loss decreased throughout the training. We decided that 10 epochs was the right value for a batch size of 64 as we realized beyond 10 epochs the loss started to increase and the accuracy decreased. The graphs for Epoch vs. accuracy/loss are shown below.\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 154,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 153,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 159,\n        columnNumber: 21\n      }, this), \"Accuracy and Loss\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 159,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat3,\n      src: epoch,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 160,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: [\"After evaluating our model with the testing dataset, we were able to achieve a rate of 94.2%. We then plotted some images along with their respective true and predicted labels to see the performance of our trained neural network. Shown below are 25 of correct predictions and 25 of wrong predictions. The wrongly predicted images show that our network struggled to distinguish between 1 and 7 and missed some digits at times. There seem to be some errors in the original labelling, which could have affected our model.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 165,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 163,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 162,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: \"Correct Predictions\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 169,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat3,\n      src: correct,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 170,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Typography, {\n      children: [/*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 171,\n        columnNumber: 21\n      }, this), \"Wrong Predictions\"]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 171,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n      className: classes.imageFormat3,\n      src: wrong,\n      alt: \"...\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 172,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: [\"These results are as expected, as the image resolution is not high, and with the images taken at different angles, it could even be hard for a human to recognize the number shown. Since we are not trying to detect the digits one by one, we don't expect our model to predict the correct number of digits for every image, as background details could be misidentified as a number potentially. Compared to a naive approach, which could perhaps make random decisions to predict the digits, our model performs much better. If the algorithm is to randomly predict the digits, the rate would be 1/(11)^N for N digits as there are a total of 11 digit possibilities (0 to 9, and None).\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 177,\n          columnNumber: 17\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 175,\n        columnNumber: 17\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 174,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(Box, {\n      className: classes.boxFormat,\n      children: [/*#__PURE__*/_jsxDEV(Typography, {\n        align: \"center\",\n        variant: \"h4\",\n        className: classes.lateTitleFormat,\n        children: \"Conclusion and Future Work\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 182,\n        columnNumber: 17\n      }, this), /*#__PURE__*/_jsxDEV(Typography, {\n        align: \"left\",\n        variant: \"subtitle1\",\n        className: classes.titleParagraphFormat2,\n        children: [\"Being the first CNN we have designed from scratch, we are reasonably satisfied with an accuracy of 94.2%. For future work, we believe that our model can be extended and the parameters can be tuned for better accuracy. As time was limited, we did not tune the parameters as much as we would have liked. Preprocessing could also be improved by improving the potential lighting/angle of the images. By preprocessing the images further, we could enhance some of the details of the digits and reduce the impact of background details on training. Having some problems predicting the correct number of digits was a problem as our model does not predict digit by digit but rather the entire number at once. Overall, we believe that this project has been a success.\", /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 186,\n          columnNumber: 773\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 186,\n          columnNumber: 782\n        }, this), /*#__PURE__*/_jsxDEV(\"br\", {}, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 186,\n          columnNumber: 791\n        }, this)]\n      }, void 0, true, {\n        fileName: _jsxFileName,\n        lineNumber: 185,\n        columnNumber: 17\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 181,\n      columnNumber: 9\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 71,\n    columnNumber: 9\n  }, this);\n};\n\n_s(FinalUpdate, \"8g5FPXexvSEOsxdmU7HicukHGqY=\", false, function () {\n  return [useStyles];\n});\n\n_c = FinalUpdate;\nexport default FinalUpdate;\n\nvar _c;\n\n$RefreshReg$(_c, \"FinalUpdate\");","map":{"version":3,"sources":["/Users/brian/Desktop/CS4641/speech-activity-recognition/src/pages/FinalUpdate/FinalUpdate.js"],"names":["React","Typography","Box","makeStyles","Header","teaser","oneHot","softmax","model","train","epoch","correct","wrong","useStyles","theme","wrapper","display","alignItems","flexDirection","paddingTop","link","textDecoration","color","boxFormat","width","titleFormat","paddingBottom","lateTitleFormat","titleParagraphFormat","fontFamily","titleParagraphFormat2","padding","imageFormat","imageFormat2","imageFormat3","FinalUpdate","tagChange","classes"],"mappings":";;;AAAA,OAAOA,KAAP,MAAkB,OAAlB;AAEA,SAASC,UAAT,EAAqBC,GAArB,QAAgC,mBAAhC;AACA,SAASC,UAAT,QAA2B,0BAA3B;AACA,SAASC,MAAT,QAAuB,kBAAvB,C,CACA;;AAGA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,MAAP,MAAmB,sBAAnB;AACA,OAAOC,OAAP,MAAoB,uBAApB;AACA,OAAOC,KAAP,MAAkB,qBAAlB;AACA,OAAOC,KAAP,MAAkB,qBAAlB;AACA,OAAOC,KAAP,MAAkB,qBAAlB;AACA,OAAOC,OAAP,MAAoB,uBAApB;AACA,OAAOC,KAAP,MAAkB,qBAAlB;;AAEA,MAAMC,SAAS,GAAGV,UAAU,CAAEW,KAAD,KAAY;AACrCC,EAAAA,OAAO,EAAE;AACLC,IAAAA,OAAO,EAAE,MADJ;AAELC,IAAAA,UAAU,EAAE,QAFP;AAGLC,IAAAA,aAAa,EAAC,QAHT;AAILC,IAAAA,UAAU,EAAC;AAJN,GAD4B;AAOrCC,EAAAA,IAAI,EAAE;AACFC,IAAAA,cAAc,EAAE,MADd;AAEFC,IAAAA,KAAK,EAAE;AAFL,GAP+B;AAWrCC,EAAAA,SAAS,EAAE;AACPC,IAAAA,KAAK,EAAE,KADA,CAEP;;AAFO,GAX0B;AAerCC,EAAAA,WAAW,EAAE;AACTC,IAAAA,aAAa,EAAC,MADL;AAETL,IAAAA,cAAc,EAAE,MAFP;AAGTC,IAAAA,KAAK,EAAC,SAHG,CAIT;;AAJS,GAfwB;AAqBrCK,EAAAA,eAAe,EAAE;AACbR,IAAAA,UAAU,EAAE,MADC;AAEbO,IAAAA,aAAa,EAAC,MAFD;AAGbL,IAAAA,cAAc,EAAE,MAHH;AAIbC,IAAAA,KAAK,EAAC,SAJO,CAKb;;AALa,GArBoB;AA4BrCM,EAAAA,oBAAoB,EAAE;AAClBC,IAAAA,UAAU,EAAE;AADM,GA5Be;AA+BrCC,EAAAA,qBAAqB,EAAE;AACnBD,IAAAA,UAAU,EAAE,eADO;AAEnBE,IAAAA,OAAO,EAAC;AAFW,GA/Bc;AAmCrCC,EAAAA,WAAW,EAAE,CACT;AADS,GAnCwB;AAsCrCC,EAAAA,YAAY,EAAE;AACVP,IAAAA,aAAa,EAAC;AADJ,GAtCuB;AAyCrCQ,EAAAA,YAAY,EAAE;AACVf,IAAAA,UAAU,EAAC;AADD;AAzCuB,CAAZ,CAAD,CAA5B;;AA8CA,MAAMgB,WAAW,GAAG,CAAC;AAAEC,EAAAA;AAAF,CAAD,KAAmB;AAAA;;AACnC,QAAMC,OAAO,GAAGxB,SAAS,EAAzB,CADmC,CAEnC;;AAIA,sBACI;AAAK,IAAA,SAAS,EAAEwB,OAAO,CAACtB,OAAxB;AAAA,4BACI,QAAC,MAAD;AAAA;AAAA;AAAA;AAAA,YADJ,eAEI,QAAC,GAAD;AAAK,MAAA,SAAS,EAAEsB,OAAO,CAACd,SAAxB;AAAA,8BAGI,QAAC,UAAD;AAAY,QAAA,KAAK,EAAG,QAApB;AAA6B,QAAA,OAAO,EAAG,IAAvC;AAA4C,QAAA,SAAS,EAAEc,OAAO,CAACZ,WAA/D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAHJ,eAMI,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEY,OAAO,CAACT,oBAAhE;AAAA,wWAE0L;AAAA;AAAA;AAAA;AAAA,gBAF1L,eAEmM;AAAA;AAAA;AAAA;AAAA,gBAFnM;AAAA;AAAA;AAAA;AAAA;AAAA,cANJ;AAAA;AAAA;AAAA;AAAA;AAAA,YAFJ,eAaI;AAAK,MAAA,SAAS,EAAES,OAAO,CAACL,WAAxB;AAAqC,MAAA,GAAG,EAAE3B,MAA1C;AAAkD,MAAA,GAAG,EAAC;AAAtD;AAAA;AAAA;AAAA;AAAA,YAbJ,eAcI,QAAC,GAAD;AAAK,MAAA,SAAS,EAAEgC,OAAO,CAACd,SAAxB;AAAA,8BACI,QAAC,UAAD;AAAY,QAAA,KAAK,EAAG,QAApB;AAA6B,QAAA,OAAO,EAAG,IAAvC;AAA4C,QAAA,SAAS,EAAEc,OAAO,CAACV,eAA/D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADJ,eAII,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEU,OAAO,CAACP,qBAAhE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAJJ,eAqBI,QAAC,UAAD;AAAY,QAAA,KAAK,EAAG,QAApB;AAA6B,QAAA,OAAO,EAAG,IAAvC;AAA4C,QAAA,SAAS,EAAEO,OAAO,CAACV,eAA/D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cArBJ,eAwBI,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEU,OAAO,CAACP,qBAAhE;AAAA,mFAC+C;AAAG,UAAA,IAAI,EAAC,yCAAR;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gBAD/C,otBACk1B;AAAA;AAAA;AAAA;AAAA,gBADl1B,eAC21B;AAAA;AAAA;AAAA;AAAA,gBAD31B,qoDAE8mD;AAAA;AAAA;AAAA;AAAA,gBAF9mD;AAAA;AAAA;AAAA;AAAA;AAAA,cAxBJ;AAAA;AAAA;AAAA;AAAA;AAAA,YAdJ,eA2CA,QAAC,UAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YA3CA,eA4CA;AAAK,MAAA,SAAS,EAAEO,OAAO,CAACL,WAAxB;AAAqC,MAAA,GAAG,EAAE1B,MAA1C;AAAkD,MAAA,GAAG,EAAC;AAAtD;AAAA;AAAA;AAAA;AAAA,YA5CA,eA6CA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAE+B,OAAO,CAACd,SAAxB;AAAA,6BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEc,OAAO,CAACP,qBAAhE;AAAA,iWAGkE;AAAA;AAAA;AAAA;AAAA,gBAHlE;AAAA;AAAA;AAAA;AAAA;AAAA;AADR;AAAA;AAAA;AAAA;AAAA,YA7CA,eAoDA,QAAC,UAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YApDA,eAqDA;AAAK,MAAA,SAAS,EAAEO,OAAO,CAACJ,YAAxB;AAAsC,MAAA,GAAG,EAAE1B,OAA3C;AAAoD,MAAA,GAAG,EAAC;AAAxD;AAAA;AAAA;AAAA;AAAA,YArDA,eAsDA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAE8B,OAAO,CAACd,SAAxB;AAAA,6BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEc,OAAO,CAACP,qBAAhE;AAAA,syCAEA;AAAA;AAAA;AAAA;AAAA,gBAFA,eAGA;AAAA;AAAA;AAAA;AAAA,gBAHA,wvBAKA;AAAA;AAAA;AAAA;AAAA,gBALA;AAAA;AAAA;AAAA;AAAA;AAAA;AADR;AAAA;AAAA;AAAA;AAAA,YAtDA,eAgEA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAIO,OAAO,CAACd,SAA1B;AAAA,8BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAG,QAApB;AAA6B,QAAA,OAAO,EAAG,IAAvC;AAA4C,QAAA,SAAS,EAAEc,OAAO,CAACV,eAA/D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADR,eAIQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEU,OAAO,CAACP,qBAAhE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAJR;AAAA;AAAA;AAAA;AAAA;AAAA,YAhEA,eAwEA,QAAC,UAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAxEA,eAyEA;AAAK,MAAA,SAAS,EAAEO,OAAO,CAACH,YAAxB;AAAsC,MAAA,GAAG,EAAE1B,KAA3C;AAAkD,MAAA,GAAG,EAAC;AAAtD;AAAA;AAAA;AAAA;AAAA,YAzEA,eA0EA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAI6B,OAAO,CAACd,SAA1B;AAAA,6BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEc,OAAO,CAACP,qBAAhE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AADR;AAAA;AAAA;AAAA;AAAA,YA1EA,eA+EA,QAAC,UAAD;AAAA,8BAAY;AAAA;AAAA;AAAA;AAAA,cAAZ;AAAA;AAAA;AAAA;AAAA;AAAA,YA/EA,eAgFA;AAAK,MAAA,SAAS,EAAEO,OAAO,CAACH,YAAxB;AAAsC,MAAA,GAAG,EAAEzB,KAA3C;AAAkD,MAAA,GAAG,EAAC;AAAtD;AAAA;AAAA;AAAA;AAAA,YAhFA,eAkFA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAI4B,OAAO,CAACd,SAA1B;AAAA,6BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEc,OAAO,CAACP,qBAAhE;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AADR;AAAA;AAAA;AAAA;AAAA,YAlFA,eAwFA,QAAC,UAAD;AAAA,8BAAY;AAAA;AAAA;AAAA;AAAA,cAAZ;AAAA;AAAA;AAAA;AAAA;AAAA,YAxFA,eAyFA;AAAK,MAAA,SAAS,EAAEO,OAAO,CAACH,YAAxB;AAAsC,MAAA,GAAG,EAAExB,KAA3C;AAAkD,MAAA,GAAG,EAAC;AAAtD;AAAA;AAAA;AAAA;AAAA,YAzFA,eA2FA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAI2B,OAAO,CAACd,SAA1B;AAAA,6BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEc,OAAO,CAACP,qBAAhE;AAAA,0iBAEA;AAAA;AAAA;AAAA;AAAA,gBAFA;AAAA;AAAA;AAAA;AAAA;AAAA;AADR;AAAA;AAAA;AAAA;AAAA,YA3FA,eAkGA,QAAC,UAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,YAlGA,eAmGA;AAAK,MAAA,SAAS,EAAEO,OAAO,CAACH,YAAxB;AAAsC,MAAA,GAAG,EAAEvB,OAA3C;AAAoD,MAAA,GAAG,EAAC;AAAxD;AAAA;AAAA;AAAA;AAAA,YAnGA,eAoGA,QAAC,UAAD;AAAA,8BAAY;AAAA;AAAA;AAAA;AAAA,cAAZ;AAAA;AAAA;AAAA;AAAA;AAAA,YApGA,eAqGA;AAAK,MAAA,SAAS,EAAE0B,OAAO,CAACH,YAAxB;AAAsC,MAAA,GAAG,EAAEtB,KAA3C;AAAkD,MAAA,GAAG,EAAC;AAAtD;AAAA;AAAA;AAAA;AAAA,YArGA,eAuGA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAIyB,OAAO,CAACd,SAA1B;AAAA,6BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEc,OAAO,CAACP,qBAAhE;AAAA,usBAEA;AAAA;AAAA;AAAA;AAAA,gBAFA;AAAA;AAAA;AAAA;AAAA;AAAA;AADR;AAAA;AAAA;AAAA;AAAA,YAvGA,eA8GA,QAAC,GAAD;AAAK,MAAA,SAAS,EAAIO,OAAO,CAACd,SAA1B;AAAA,8BACQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAG,QAApB;AAA6B,QAAA,OAAO,EAAG,IAAvC;AAA4C,QAAA,SAAS,EAAEc,OAAO,CAACV,eAA/D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cADR,eAIQ,QAAC,UAAD;AAAY,QAAA,KAAK,EAAC,MAAlB;AAAyB,QAAA,OAAO,EAAC,WAAjC;AAA6C,QAAA,SAAS,EAAEU,OAAO,CAACP,qBAAhE;AAAA,wxBACovB;AAAA;AAAA;AAAA;AAAA,gBADpvB,eAC6vB;AAAA;AAAA;AAAA;AAAA,gBAD7vB,eACswB;AAAA;AAAA;AAAA;AAAA,gBADtwB;AAAA;AAAA;AAAA;AAAA;AAAA,cAJR;AAAA;AAAA;AAAA;AAAA;AAAA,YA9GA;AAAA;AAAA;AAAA;AAAA;AAAA,UADJ;AA4HH,CAlID;;GAAMK,W;UACctB,S;;;KADdsB,W;AAoIN,eAAeA,WAAf","sourcesContent":["import React from 'react';\n\nimport { Typography, Box } from '@material-ui/core';\nimport { makeStyles } from '@material-ui/core/styles';\nimport { Header } from '../../components';\n//import { Link } from 'react-router-dom';\n\n\nimport teaser from \"../../img/teaser.png\";\nimport oneHot from \"../../img/oneHot.png\";\nimport softmax from \"../../img/softmax.png\";\nimport model from \"../../img/model.png\";\nimport train from \"../../img/train.png\";\nimport epoch from \"../../img/epoch.png\";\nimport correct from \"../../img/correct.png\";\nimport wrong from \"../../img/wrong.png\";\n\nconst useStyles = makeStyles((theme) => ({\n    wrapper: {\n        display: 'flex',\n        alignItems: 'center',\n        flexDirection:'column',\n        paddingTop:'20px'\n    },\n    link: {\n        textDecoration: 'none',\n        color: '#000',\n    },\n    boxFormat: {\n        width: '70%',\n        //paddingBottom:'20px',\n    },\n    titleFormat: {\n        paddingBottom:'10px',\n        textDecoration: \"none\",\n        color:'#212F3C',\n        //fontFamily: '-apple-system',\n    },\n    lateTitleFormat: {\n        paddingTop: '20px',\n        paddingBottom:'10px',\n        textDecoration: \"none\",\n        color:'#212F3C',\n        //fontFamily: '-apple-system',\n    },\n    titleParagraphFormat: {\n        fontFamily: '-apple-system',\n    },\n    titleParagraphFormat2: {\n        fontFamily: '-apple-system',\n        padding:'10px',\n    },\n    imageFormat: {\n        //paddingBottom:'20px',\n    },\n    imageFormat2: {\n        paddingBottom:'30px',\n    },\n    imageFormat3: {\n        paddingTop:'20px',\n    },\n}));\n\nconst FinalUpdate = ({ tagChange }) => {\n    const classes = useStyles();\n    //const theme = useTheme();\n\n\n\n    return (\n        <div className={classes.wrapper}>\n            <Header></Header>\n            <Box className={classes.boxFormat}>\n\n\n                <Typography align = 'center' variant = 'h4' className={classes.titleFormat}>\n                    Abstract\n                </Typography>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat}>\n                The goal of our project is to train a convolutional neural network (CNN) to detect the digits of a house number displayed on the streets. \n                This is a basic image recognition that can be useful in many different fields. We were able to train our neural network to perform multi-digit recognition with an accuracy rate of 94.2%.<br></br><br></br>\n                </Typography>\n            </Box>\n            <img className={classes.imageFormat} src={teaser} alt=\"...\"></img>\n            <Box className={classes.boxFormat}>\n                <Typography align = 'center' variant = 'h4' className={classes.lateTitleFormat}>\n                    Introduction\n                </Typography>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                As shown in the image above, our project targets images of house numbers displayed on the streets, \n                we hope to achieve reasonable results given a variety of resolutions of the images. \n                Digit recognition is becoming increasingly important in various domains as technology advances. \n                Whether it is a mapping company needing to match images of house numbers to their geolocations, \n                or a robot trying to locate itself through room numbers/house numbers, \n                SVHN serves as a good basic dataset to dive into the world of image recognition. \n                We wanted to implement our project from scratch to understand how to preprocess the dataset as well as \n                to gain a deeper understanding of how to set up and train our own neural network. \n                Most existing ways of recognizing street view house numbers include two steps after locating where the numbers are within a picture: \n                slicing each digit and recognizing. The ways some other projects built the neural network rely on preprocessing the picture to make boxes around each digit, \n                and can only recognize individual digits. The output layers of their convolutional neural networks have 11 neurons, \n                each corresponding to a possible digit (ten of which represent zero to nine, and the last one represent none). \n                Our approach only has one step after locating the numbers in the pictures. We directly feed the portion of the image with all the numbers into the network, \n                and train the network to predict all of the numbers at the same time.\n                </Typography>\n\n                <Typography align = 'center' variant = 'h4' className={classes.lateTitleFormat}>\n                    Approach\n                </Typography>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                The first step was to preprocess the data from <a href=\"http://ufldl.stanford.edu/housenumbers/\">The Street View House Numbers (SVHN) Dataset</a>. We first focused on the first part of the dataset, where colored house-number images are given, and the bounding box for each digit is given in a .mat file. The general steps we took to preprocess the image dataset are to: limit the maximum number of digit to 5 (there is only one image that has 6 digits in the picture), make a new bounding box given the min and max values of the x and y values for all bounding boxes of all digits, expand the bounding box in each direction by 30% to ensure the coverage of all digit details. After preprocessing all the images of the dataset, we chose to use Tensorflow.Keras to help us construct the CNN as it has many predefined methods to build the architecture. <br></br><br></br>\n                As introduced in the midterm update, we were able to preprocess the images and store them in a .h5 file. The data contains cropped street view house number images of size 32x32 and their corresponding labels in the form of 5 elements lists. Each of these label lists contains 5 integers, ranging from 0 to 10, with 0 to 9 representing the actual number shown in the image, and 10 being a placeholder for images that have less than 5 numbers (If \"19\" appears in the image, the label list would be: [1, 9, 10, 10, 10]). To construct the CNN architecture, we began by defining the input and output layers. Our input would be the preprocessed images, which have size 32x32x3. Finding the way to construct the output layer was less intuitive for us. Common classification CNN selects and condenses important features in hidden layers, and makes the number of neurons on the output layer correspond to the number of classes the data belongs to. In this case, as each element in our labels ranges from 0 to 10, there should be 11 classes, which correspond to 11 neurons on the output layer, and the list containing the correct information for the output layer to compare with should contain 11 elements. This does not coincide with our label data set, as each of these label lists contain 5 elements. Therefore, we first further processed our labels and converted them using the one-hot encoding method. Each of these 5x1 label lists then becomes a 5x11 list, with ten 0s and one 1 located on the index corresponding to this label. Shown below is an example of how the label \"19\" is represented in the original label list as well as the one-hot format. <br></br>\n                </Typography>\n        </Box>\n        <Typography>One-Hot Encoding</Typography>\n        <img className={classes.imageFormat} src={oneHot} alt=\"...\"></img>\n        <Box className={classes.boxFormat}>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                With the help of this, we further designed the output layer of the network to also be in the dimension of 5x11. \n                We individually connected 5 separate softmax densely-connected layers with 11 neurons each to their previous common fully connected layer. \n                Then we concatenated them and reshaped them into the size of 5x11.<br></br>\n                </Typography>\n        </Box>\n        <Typography>Design of Output Layer</Typography>\n        <img className={classes.imageFormat2} src={softmax} alt=\"...\"></img>\n        <Box className={classes.boxFormat}>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                After settling down our input and output layers, we started to construct the hidden layers. Considering that our dataset is only recognizing numbers, which do not have very complex featured shapes and patterns, but there are 11 possible categories, we decided to use 3 convolutional layers, each is followed by 3 pooling layers, and there are 2 fully connected layers at the end. For each of these convolutional layers, we used Rectified Linear Unit(ReLU) as our activation function, as it allows an arbitrary amount of output, which corresponds to our scenario of wanting to classify into many classes. The convolutional layers have 64, 128, 256 filters, with a 7x7, 5x5, 3x3 dimension of kernel respectively. As the dimension of the kernel represents the height and width of the 2D convolution window, we decided to gradually decrease it while increasing the amount of filters to capture more essential details. As for padding, we wanted the results coming out from the first layer to preserve spatial dimensions of the volume, so we set padding to be the same for the first layer. For the second and the third, as the layers gradually identified important features, we set the padding to valid to allow the spatial dimensions to reduce via the natural application of convolution.\n                <br></br>\n                <br></br>\n                Before each of these convolutional layers, we performed a batch normalization to preprocess their input, reduce the internal covariate shift, and increase the learning efficiencies. Following each convolutional layer, we used Max Pooling with a size of 2x2 to reduce the spatial dimensions of the output volume. After this, we wanted to increase the challenges for the network to learn the data by randomly dropping out some neurons on each layer, so we made each of these max pooled layers dropout with a rate of [0.3, 0.3, 0.5] respectively. For the fully connected layers. We first flattened the data and then applied two fully connected layers with sizes 1024 and 512, using them to combine the features and prepare for the final output.\n                <br></br>\n                </Typography>\n\n        </Box>\n        <Box className = {classes.boxFormat}>\n                <Typography align = 'center' variant = 'h4' className={classes.lateTitleFormat}>\n                    Experimental and Qualitative Results\n                </Typography>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                The input to the CNN is a set of 32x32x3 images, we conducted trials on black and white images first and found similar rates of success, we will be displaying the results of training and testing on colored images as this is more realistic and we do not expect users to preprocess images to black and white before using our model. Shown below is an image of the neural network model summary, showing the various layers and shapes.\n                </Typography>\n        </Box>\n        <Typography>Model Summary</Typography>\n        <img className={classes.imageFormat3} src={model} alt=\"...\"></img>\n        <Box className = {classes.boxFormat}>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                After setting up our neural network as above, we spent a lot of time tuning the various things such as the number of drop out layers, the batch size, and the number of epochs ran. We found out that having more dropout layers increases the accuracy as the act of randomly dropping neurons increases the challenge for the network to learn the data, making the network record more essential details, and thus increasing its ability for generalization. The batch size was closely related to the number of epochs, as we realized that for a smaller batch size, a smaller value for epoch should be chosen for higher accuracy. We tested batch sizes of 16 and 64, varying the epoch value between 3, 19, 15, and 20. In the end, we chose a batch size of 64 and epoch value of 10 with the lowest loss towards the end and maximum accuracy. We tested two optimizers: Adam and Root Mean Square Propagation (RMSProp). We chose RMPSprop in the end as it tries to dampen the oscillations and automatically adjust the learning rates.\n                </Typography>\n        </Box>\n        <Typography><br></br>Training Process</Typography>\n        <img className={classes.imageFormat3} src={train} alt=\"...\"></img>\n        \n        <Box className = {classes.boxFormat}>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                As shown above, the accuracy increased as the loss decreased throughout the training. We decided that 10 epochs was the right value for a batch size of 64 as we realized beyond 10 epochs the loss started to increase and the accuracy decreased. The graphs for Epoch vs. accuracy/loss are shown below.\n                </Typography>\n        </Box>\n\n        <Typography><br></br>Accuracy and Loss</Typography>\n        <img className={classes.imageFormat3} src={epoch} alt=\"...\"></img>\n\n        <Box className = {classes.boxFormat}>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                After evaluating our model with the testing dataset, we were able to achieve a rate of 94.2%. We then plotted some images along with their respective true and predicted labels to see the performance of our trained neural network. Shown below are 25 of correct predictions and 25 of wrong predictions. The wrongly predicted images show that our network struggled to distinguish between 1 and 7 and missed some digits at times. There seem to be some errors in the original labelling, which could have affected our model.\n                <br></br>\n                </Typography>\n        </Box>\n\n        <Typography>Correct Predictions</Typography>\n        <img className={classes.imageFormat3} src={correct} alt=\"...\"></img>\n        <Typography><br></br>Wrong Predictions</Typography>\n        <img className={classes.imageFormat3} src={wrong} alt=\"...\"></img>\n\n        <Box className = {classes.boxFormat}>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                These results are as expected, as the image resolution is not high, and with the images taken at different angles, it could even be hard for a human to recognize the number shown. Since we are not trying to detect the digits one by one, we don't expect our model to predict the correct number of digits for every image, as background details could be misidentified as a number potentially. Compared to a naive approach, which could perhaps make random decisions to predict the digits, our model performs much better. If the algorithm is to randomly predict the digits, the rate would be 1/(11)^N for N digits as there are a total of 11 digit possibilities (0 to 9, and None).\n                <br></br>\n                </Typography>\n        </Box>\n\n        <Box className = {classes.boxFormat}>\n                <Typography align = 'center' variant = 'h4' className={classes.lateTitleFormat}>\n                    Conclusion and Future Work\n                </Typography>\n                <Typography align='left' variant='subtitle1' className={classes.titleParagraphFormat2}>\n                Being the first CNN we have designed from scratch, we are reasonably satisfied with an accuracy of 94.2%. For future work, we believe that our model can be extended and the parameters can be tuned for better accuracy. As time was limited, we did not tune the parameters as much as we would have liked. Preprocessing could also be improved by improving the potential lighting/angle of the images. By preprocessing the images further, we could enhance some of the details of the digits and reduce the impact of background details on training. Having some problems predicting the correct number of digits was a problem as our model does not predict digit by digit but rather the entire number at once. Overall, we believe that this project has been a success.<br></br><br></br><br></br>\n                </Typography>\n        </Box>\n        \n\n        \n        </div>\n    )\n}\n\nexport default FinalUpdate;"]},"metadata":{},"sourceType":"module"}